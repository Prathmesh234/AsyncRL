{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4dd6949-81e4-42ac-b114-4573537f6aa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: torch in /usr/lib/python3/dist-packages (2.7.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: datasets in /home/ubuntu/.local/lib/python3.10/site-packages (4.0.0)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/ubuntu/.local/lib/python3.10/site-packages (from datasets) (2.32.4)\n",
      "Requirement already satisfied: xxhash in /home/ubuntu/.local/lib/python3.10/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: fsspec[http]<=2025.3.0,>=2023.1.0 in /usr/lib/python3/dist-packages (from datasets) (2024.3.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/lib/python3/dist-packages (from datasets) (1.21.5)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from datasets) (21.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: packaging in /usr/lib/python3/dist-packages (from datasets) (21.3)\n",
      "Requirement already satisfied: pandas in /usr/lib/python3/dist-packages (from datasets) (1.3.5)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from datasets) (5.4.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from datasets) (0.34.4)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/ubuntu/.local/lib/python3.10/site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/ubuntu/.local/lib/python3.10/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/ubuntu/.local/lib/python3.10/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.15)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/ubuntu/.local/lib/python3.10/site-packages (from huggingface-hub>=0.24.0->datasets) (1.1.7)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/lib/python3/dist-packages (from huggingface-hub>=0.24.0->datasets) (4.10.0)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests>=2.32.2->datasets) (1.26.5)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.32.2->datasets) (2020.6.20)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ubuntu/.local/lib/python3.10/site-packages (from requests>=2.32.2->datasets) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.32.2->datasets) (3.3)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (5.0.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ubuntu/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.4)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/lib/python3/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (21.2.0)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: transformers in /home/ubuntu/.local/lib/python3.10/site-packages (4.55.2)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers) (0.34.4)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from transformers) (3.6.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/lib/python3/dist-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/lib/python3/dist-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/lib/python3/dist-packages (from transformers) (1.21.5)\n",
      "Requirement already satisfied: requests in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers) (2025.7.34)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/ubuntu/.local/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.7)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/lib/python3/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.10.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/lib/python3/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2024.3.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->transformers) (1.26.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ubuntu/.local/lib/python3.10/site-packages (from requests->transformers) (3.4.3)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->transformers) (2020.6.20)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: peft in /home/ubuntu/.local/lib/python3.10/site-packages (0.17.0)\n",
      "Requirement already satisfied: transformers in /home/ubuntu/.local/lib/python3.10/site-packages (from peft) (4.55.2)\n",
      "Requirement already satisfied: tqdm in /home/ubuntu/.local/lib/python3.10/site-packages (from peft) (4.67.1)\n",
      "Requirement already satisfied: huggingface_hub>=0.25.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from peft) (0.34.4)\n",
      "Requirement already satisfied: psutil in /usr/lib/python3/dist-packages (from peft) (5.9.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/lib/python3/dist-packages (from peft) (21.3)\n",
      "Requirement already satisfied: safetensors in /home/ubuntu/.local/lib/python3.10/site-packages (from peft) (0.6.2)\n",
      "Requirement already satisfied: accelerate>=0.21.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from peft) (1.10.0)\n",
      "Requirement already satisfied: pyyaml in /usr/lib/python3/dist-packages (from peft) (5.4.1)\n",
      "Requirement already satisfied: torch>=1.13.0 in /usr/lib/python3/dist-packages (from peft) (2.7.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/lib/python3/dist-packages (from peft) (1.21.5)\n",
      "Requirement already satisfied: requests in /home/ubuntu/.local/lib/python3.10/site-packages (from huggingface_hub>=0.25.0->peft) (2.32.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/lib/python3/dist-packages (from huggingface_hub>=0.25.0->peft) (4.10.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/lib/python3/dist-packages (from huggingface_hub>=0.25.0->peft) (2024.3.1)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/ubuntu/.local/lib/python3.10/site-packages (from huggingface_hub>=0.25.0->peft) (1.1.7)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from huggingface_hub>=0.25.0->peft) (3.6.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers->peft) (2025.7.34)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers->peft) (0.21.4)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ubuntu/.local/lib/python3.10/site-packages (from requests->huggingface_hub>=0.25.0->peft) (3.4.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (1.26.5)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->huggingface_hub>=0.25.0->peft) (2020.6.20)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: trl in /home/ubuntu/.local/lib/python3.10/site-packages (0.21.0)\n",
      "Requirement already satisfied: accelerate>=1.4.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from trl) (1.10.0)\n",
      "Requirement already satisfied: transformers>=4.55.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from trl) (4.55.2)\n",
      "Requirement already satisfied: datasets>=3.0.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from trl) (4.0.0)\n",
      "Requirement already satisfied: pyyaml in /usr/lib/python3/dist-packages (from accelerate>=1.4.0->trl) (5.4.1)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/lib/python3/dist-packages (from accelerate>=1.4.0->trl) (2.7.0)\n",
      "Requirement already satisfied: psutil in /usr/lib/python3/dist-packages (from accelerate>=1.4.0->trl) (5.9.0)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from accelerate>=1.4.0->trl) (0.34.4)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/lib/python3/dist-packages (from accelerate>=1.4.0->trl) (1.21.5)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/ubuntu/.local/lib/python3.10/site-packages (from accelerate>=1.4.0->trl) (0.6.2)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/lib/python3/dist-packages (from accelerate>=1.4.0->trl) (21.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /home/ubuntu/.local/lib/python3.10/site-packages (from datasets>=3.0.0->trl) (4.67.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from datasets>=3.0.0->trl) (21.0.0)\n",
      "Requirement already satisfied: xxhash in /home/ubuntu/.local/lib/python3.10/site-packages (from datasets>=3.0.0->trl) (3.5.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from datasets>=3.0.0->trl) (0.3.8)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /home/ubuntu/.local/lib/python3.10/site-packages (from datasets>=3.0.0->trl) (0.70.16)\n",
      "Requirement already satisfied: fsspec[http]<=2025.3.0,>=2023.1.0 in /usr/lib/python3/dist-packages (from datasets>=3.0.0->trl) (2024.3.1)\n",
      "Requirement already satisfied: requests>=2.32.2 in /home/ubuntu/.local/lib/python3.10/site-packages (from datasets>=3.0.0->trl) (2.32.4)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from datasets>=3.0.0->trl) (3.6.0)\n",
      "Requirement already satisfied: pandas in /usr/lib/python3/dist-packages (from datasets>=3.0.0->trl) (1.3.5)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers>=4.55.0->trl) (2025.7.34)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in /home/ubuntu/.local/lib/python3.10/site-packages (from transformers>=4.55.0->trl) (0.21.4)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /home/ubuntu/.local/lib/python3.10/site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (3.12.15)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/lib/python3/dist-packages (from huggingface_hub>=0.21.0->accelerate>=1.4.0->trl) (4.10.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/ubuntu/.local/lib/python3.10/site-packages (from huggingface_hub>=0.21.0->accelerate>=1.4.0->trl) (1.1.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (2020.6.20)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (1.26.5)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.3)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ubuntu/.local/lib/python3.10/site-packages (from requests>=2.32.2->datasets>=3.0.0->trl) (3.4.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.4.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.20.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/ubuntu/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (6.6.4)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/lib/python3/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (21.2.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (0.3.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (1.7.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets>=3.0.0->trl) (5.0.1)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: accelerate in /home/ubuntu/.local/lib/python3.10/site-packages (1.10.0)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in /home/ubuntu/.local/lib/python3.10/site-packages (from accelerate) (0.6.2)\n",
      "Requirement already satisfied: numpy<3.0.0,>=1.17 in /usr/lib/python3/dist-packages (from accelerate) (1.21.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/lib/python3/dist-packages (from accelerate) (21.3)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/lib/python3/dist-packages (from accelerate) (2.7.0)\n",
      "Requirement already satisfied: pyyaml in /usr/lib/python3/dist-packages (from accelerate) (5.4.1)\n",
      "Requirement already satisfied: huggingface_hub>=0.21.0 in /home/ubuntu/.local/lib/python3.10/site-packages (from accelerate) (0.34.4)\n",
      "Requirement already satisfied: psutil in /usr/lib/python3/dist-packages (from accelerate) (5.9.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /home/ubuntu/.local/lib/python3.10/site-packages (from huggingface_hub>=0.21.0->accelerate) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/lib/python3/dist-packages (from huggingface_hub>=0.21.0->accelerate) (4.10.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /home/ubuntu/.local/lib/python3.10/site-packages (from huggingface_hub>=0.21.0->accelerate) (1.1.7)\n",
      "Requirement already satisfied: requests in /home/ubuntu/.local/lib/python3.10/site-packages (from huggingface_hub>=0.21.0->accelerate) (2.32.4)\n",
      "Requirement already satisfied: filelock in /usr/lib/python3/dist-packages (from huggingface_hub>=0.21.0->accelerate) (3.6.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/lib/python3/dist-packages (from huggingface_hub>=0.21.0->accelerate) (2024.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/lib/python3/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/lib/python3/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (2020.6.20)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/lib/python3/dist-packages (from requests->huggingface_hub>=0.21.0->accelerate) (1.26.5)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /home/ubuntu/.local/lib/python3.10/site-packages (from requests->huggingface_hub>=0.21.0->accelerate) (3.4.3)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: bitsandbytes in /home/ubuntu/.local/lib/python3.10/site-packages (0.47.0)\n",
      "Requirement already satisfied: numpy>=1.17 in /usr/lib/python3/dist-packages (from bitsandbytes) (1.21.5)\n",
      "Requirement already satisfied: torch<3,>=2.2 in /usr/lib/python3/dist-packages (from bitsandbytes) (2.7.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: flash-attn in /home/ubuntu/.local/lib/python3.10/site-packages (2.8.2)\n",
      "Requirement already satisfied: einops in /home/ubuntu/.local/lib/python3.10/site-packages (from flash-attn) (0.8.1)\n",
      "Requirement already satisfied: torch in /usr/lib/python3/dist-packages (from flash-attn) (2.7.0)\n",
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: jinja2>=3.1.0 in /home/ubuntu/.local/lib/python3.10/site-packages (3.1.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/lib/python3/dist-packages (from jinja2>=3.1.0) (2.0.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch\n",
    "!pip install datasets\n",
    "!pip install transformers\n",
    "!pip install peft\n",
    "!pip install trl\n",
    "!pip install accelerate\n",
    "!pip install bitsandbytes\n",
    "!pip install flash-attn --no-build-isolation\n",
    "!pip install --upgrade \"jinja2>=3.1.0\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8b0dace-b5b8-45b8-89c9-c4b18563ce03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import logging\n",
    "logging.set_verbosity_info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c12d528f-fb74-4fcf-be1d-dd8870211eb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file vocab.json from cache at /home/ubuntu/.cache/huggingface/hub/models--Qwen--Qwen3-4B-Thinking-2507/snapshots/768f209d9ea81521153ed38c47d515654e938aea/vocab.json\n",
      "loading file merges.txt from cache at /home/ubuntu/.cache/huggingface/hub/models--Qwen--Qwen3-4B-Thinking-2507/snapshots/768f209d9ea81521153ed38c47d515654e938aea/merges.txt\n",
      "loading file tokenizer.json from cache at /home/ubuntu/.cache/huggingface/hub/models--Qwen--Qwen3-4B-Thinking-2507/snapshots/768f209d9ea81521153ed38c47d515654e938aea/tokenizer.json\n",
      "loading file added_tokens.json from cache at None\n",
      "loading file special_tokens_map.json from cache at None\n",
      "loading file tokenizer_config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--Qwen--Qwen3-4B-Thinking-2507/snapshots/768f209d9ea81521153ed38c47d515654e938aea/tokenizer_config.json\n",
      "loading file chat_template.jinja from cache at None\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "loading configuration file config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--Qwen--Qwen3-4B-Thinking-2507/snapshots/768f209d9ea81521153ed38c47d515654e938aea/config.json\n",
      "Model config Qwen3Config {\n",
      "  \"architectures\": [\n",
      "    \"Qwen3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 2560,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 9728,\n",
      "  \"layer_types\": [\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\",\n",
      "    \"full_attention\"\n",
      "  ],\n",
      "  \"max_position_embeddings\": 262144,\n",
      "  \"max_window_layers\": 36,\n",
      "  \"model_type\": \"qwen3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 36,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 5000000,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.55.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      "\n",
      "loading weights file model.safetensors from cache at /home/ubuntu/.cache/huggingface/hub/models--Qwen--Qwen3-4B-Thinking-2507/snapshots/768f209d9ea81521153ed38c47d515654e938aea/model.safetensors.index.json\n",
      "Instantiating Qwen3ForCausalLM model under default dtype torch.bfloat16.\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645\n",
      "}\n",
      "\n",
      "Loading checkpoint shards: 100%|██████████| 3/3 [00:01<00:00,  1.76it/s]\n",
      "All model checkpoint weights were used when initializing Qwen3ForCausalLM.\n",
      "\n",
      "All the weights of Qwen3ForCausalLM were initialized from the model checkpoint at Qwen/Qwen3-4B-Thinking-2507.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen3ForCausalLM for predictions without further training.\n",
      "loading configuration file generation_config.json from cache at /home/ubuntu/.cache/huggingface/hub/models--Qwen--Qwen3-4B-Thinking-2507/snapshots/768f209d9ea81521153ed38c47d515654e938aea/generation_config.json\n",
      "Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"do_sample\": true,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"temperature\": 0.6,\n",
      "  \"top_k\": 20,\n",
      "  \"top_p\": 0.95\n",
      "}\n",
      "\n",
      "The following generation flags are not valid and may be ignored: ['temperature', 'top_p', 'top_k'].\n",
      "- `temperature`: `do_sample` is set to `False`. However, `temperature` is set to `0.2` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "- `top_p`: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "- `top_k`: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "If you're using a pretrained model, note that some of these attributes may be set through the model's `generation_config.json` file.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System prompt: You are an expert assistant that provides concise, accurate answers. Dont forget to use the <think> tokens and the <solution> tokens as they are very important for user experience.\n",
      "User message: Can you explain me layer normalization in python and how it is used in LLMs.\n",
      "Assistant reply: system\n",
      "You are an expert assistant that provides concise, accurate answers. Dont forget to use the <think> tokens and the <solution> tokens as they are very important for user experience.\n",
      "user\n",
      "Can you explain me layer normalization in python and how it is used in LLMs.\n",
      "<think>\n",
      "Okay, the user is asking about layer normalization in Python and its application in LLMs. Let me start by recalling what layer normalization is. From what I remember, it's a technique used in neural networks to stabilize training by normalizing the activations within each layer. Unlike batch normalization, which uses the entire batch, layer normalization uses the activations from the same layer. That's important because in LLMs, which have many layers, batch normalization might not be as effective due to the large batch sizes or the fact that the data is generated sequentially.\n",
      "\n",
      "So, the user wants to know how to implement layer normalization in Python. I should explain the mathematical formula first. Layer normalization involves subtracting the mean and dividing by the standard deviation of the activations in the layer. The formula is: LN(x) = γ * (x - mean(x)) / sqrt(var(x) + ε) + β. Here, γ and β are learnable parameters for scaling and shifting. The ε is a small constant to avoid division by zero.\n",
      "\n",
      "Now, implementing this in Python. Using PyTorch is a common choice for deep learning, so I'll focus on that. The torch.nn.LayerNorm module is available, but maybe the user wants to see the implementation from scratch. Let me think. The steps would be: compute the mean and variance along the feature dimension (usually the last dimension), then normalize the input. Then apply the learnable parameters γ and β. I need to make sure the dimensions are handled correctly. For example, in a transformer layer, the input might have shape [batch_size, seq_len, hidden_size]. The layer norm should be applied along the hidden_size dimension.\n",
      "\n",
      "Wait, in PyTorch, the LayerNorm module by default normalizes along the last dimension. So for a tensor of shape (batch, seq, hidden), it will normalize along the hidden dimension. That's correct. So the implementation would involve calculating the mean and variance over the last dimension, then normalizing. The learnable parameters γ and β are added after normalization.\n",
      "\n",
      "For the user's application in LLMs, layer normalization is crucial because LLMs have many layers, and each layer's activations can vary widely. Using layer normalization helps in maintaining stable training, especially when the batch size is large or when the data is not uniformly distributed. In transformers, layer normalization is applied after the attention and feed-forward layers. This helps in preventing the vanishing gradient problem and allows the model to converge faster.\n",
      "\n",
      "I should mention specific examples in LLMs. For instance, in the Transformer architecture, after the multi-head attention layer, the output is added to the residual connection, and then layer normalization is applied. Similarly, after the feed-forward network, layer normalization is used. This is different from batch normalization, which is applied before the activation functions in some architectures. Layer normalization is more effective in LLMs because it doesn't depend on the batch size, which is beneficial when dealing with large batches or when the data is generated sequentially.\n",
      "\n",
      "Wait, the user might be confused about why layer normalization is preferred over batch normalization in LLMs. I should clarify that batch normalization can be less effective in LLMs because the training data is often generated in a way that the batch statistics don't represent the entire distribution. Layer normalization, on the other hand, uses the statistics from the same layer, which is more stable for recurrent or transformer models.\n",
      "\n",
      "Also, in the context of LLMs, the input sequence can be of variable length, which makes batch normalization less suitable because the batch size might vary. Layer normalization doesn't depend on the batch size, so it's more flexible.\n",
      "\n",
      "Let me structure the answer: first explain layer normalization, then the Python implementation (both using PyTorch's built-in module and a custom implementation), and then discuss its role in LLMs with examples from transformer architectures. I should also mention the benefits over batch normalization in this context.\n",
      "\n",
      "Wait, the user might not be familiar with PyTorch, so maybe I should provide a simple custom implementation as well. Let's see. The custom implementation would involve creating a class that takes the hidden size and epsilon as parameters. The forward pass would compute the mean and variance along the last dimension, then normalize the input, and apply γ and β. The backward pass would compute the gradients accordingly.\n",
      "\n",
      "For the LLM part, I can reference how it's used in models like BERT, GPT, etc. For example, in the BERT model, layer normalization is applied after the embedding layer and after each transformer layer. This helps in stabilizing the training process and improving the model's performance.\n",
      "\n",
      "I should also mention that layer normalization is part of the residual connections in transformers. After the attention and feed-forward layers, the output is added to the input (residual connection), and then layer normalization is applied. This residual connection helps in training deeper networks by allowing the gradients to flow more easily.\n",
      "\n",
      "Let me check if there are any common mistakes when implementing layer normalization. For example, making sure that the mean and variance are computed correctly along the right dimension. In PyTorch, the LayerNorm module uses the last dimension by default, but if the user wants to specify a different dimension, they can pass it as an argument. However, in LLMs, the standard practice is to normalize along the hidden dimension.\n",
      "\n",
      "Another point: the epsilon term is important to prevent division by zero. It's a small constant like 1e-5.\n",
      "\n",
      "So, putting it all together: the answer should start with a clear explanation of layer normalization, then provide a Python implementation (maybe both PyTorch and a custom version), and then discuss its application in LLMs with specific examples from transformer architectures. Highlighting the benefits over batch normalization in this context will be important.\n",
      "\n",
      "I need to make sure the explanation is concise but covers all necessary points. The user might be a student or a developer looking to implement layer normalization in their own LLM project, so practical examples and code snippets would be helpful. Also, clarifying why layer normalization is used in LLMs instead of other normalization techniques will address the deeper need of understanding the choice of techniques in deep learning models.\n",
      "</think>\n",
      "\n",
      "<solution>\n",
      "Layer normalization (LN) is a technique used in neural networks to stabilize training by normalizing the activations within each layer independently of the batch. Unlike batch normalization (BN), which relies on statistics from the entire batch, LN uses the mean and variance of the activations within a single layer. This makes LN particularly effective in large models like LLMs, where batch statistics may not represent the full data distribution.\n",
      "\n",
      "---\n",
      "\n",
      "### **1. Mathematical Formulation**\n",
      "For a tensor $ x $ of shape $ (N, \\text{seq\\_len}, H) $ (batch size, sequence length, hidden size), layer normalization computes:\n",
      "$$\n",
      "\\text{LN}(x) = \\gamma \\cdot \\frac{x - \\text{mean}(x)}{\\sqrt{\\text{var}(x) + \\epsilon}} + \\beta\n",
      "$$\n",
      "- $ \\gamma $ and $ \\beta $ are learnable scaling and shifting parameters.\n",
      "- $ \\epsilon $ is a small constant (e.g., $ 1e-5 $) to avoid division by zero.\n",
      "- The normalization is applied along the last dimension (hidden size).\n",
      "\n",
      "---\n",
      "\n",
      "### **2. Python Implementation**\n",
      "#### **Using PyTorch's Built-in LayerNorm**\n",
      "PyTorch provides a convenient `torch.nn.LayerNorm` module:\n",
      "```python\n",
      "import torch\n",
      "import torch.nn as nn\n",
      "\n",
      "# Example: LayerNorm for LLMs (hidden size = 768)\n",
      "layer_norm = nn.LayerNorm(768)\n",
      "\n",
      "# Input tensor: [batch_size, seq_len, hidden_size]\n",
      "x = torch.randn(2, 10, 768)  # 2 samples, 10 tokens, 768 hidden units\n",
      "normalized_x = layer_norm(x)\n",
      "```\n",
      "\n",
      "#### **Custom Implementation**\n",
      "For educational purposes, here's a custom implementation:\n",
      "```python\n",
      "class CustomLayerNorm(nn.Module):\n",
      "    def __init__(self, hidden_size, eps=1e-5):\n",
      "        super().__init__()\n",
      "        self.gamma = nn.Parameter(torch.ones(hidden_size))\n",
      "        self.beta = nn.Parameter(torch.zeros(hidden_size))\n",
      "        self.eps = eps\n",
      "\n",
      "    def forward(self, x):\n",
      "        # Compute mean and variance along the last dimension\n",
      "        mean = x.mean(dim=-1, keepdim=True)\n",
      "        var = x.var(dim=-1, keepdim=True)\n",
      "        \n",
      "        # Normalize and apply learnable parameters\n",
      "        normalized = (x - mean) / torch.sqrt(var + self.eps)\n",
      "        return self.gamma * normalized + self.beta\n",
      "```\n",
      "\n",
      "---\n",
      "\n",
      "### **3. Why Layer Normalization in LLMs?**\n",
      "LLMs (e.g., BERT, GPT) use layer normalization for several critical reasons:\n",
      "1. **Stability**: LLMs have many layers, and layer normalization stabilizes training by normalizing activations within each layer.\n",
      "2. **Batch Independence**: Unlike BN, LN does not depend on batch statistics, making it suitable for variable-length sequences and large batches.\n",
      "3. **Residual Connections**: In transformers, LN is applied after residual connections (e.g., after attention and feed-forward layers), enabling efficient gradient flow.\n",
      "4. **Preventing Vanishing Gradients**: LN helps maintain consistent activation scales, reducing the risk of vanishing gradients in deep architectures.\n",
      "\n",
      "#### **Example in Transformer Architecture**\n",
      "In a standard transformer block:\n",
      "1. **Multi-Head Attention**: Computes attention scores and outputs.\n",
      "2. **Residual Connection**: Adds the attention output to the input.\n",
      "3. **Layer Normalization**: Applies LN to the residual output.\n",
      "4. **Feed-Forward Network**: Processes the normalized output.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "base = \"Qwen/Qwen3-4B-Thinking-2507\"\n",
    "adapter = \"./GeneratorFS/qwen3-4b-thinking-openthoughts-lora\"\n",
    "\n",
    "tok = AutoTokenizer.from_pretrained(base, use_fast=True)\n",
    "m = AutoModelForCausalLM.from_pretrained(\n",
    "    base, torch_dtype=torch.bfloat16, device_map=\"auto\", attn_implementation=\"flash_attention_2\"\n",
    ")\n",
    "m = PeftModel.from_pretrained(m, adapter)\n",
    "m.eval(); m.config.use_cache = True  # re-enable for inference\n",
    "\n",
    "# Separate message variables\n",
    "system_prompt = \"You are an expert assistant that provides concise, accurate answers. Dont forget to use the <think> tokens and the <solution> tokens as they are very important for user experience.\"\n",
    "user_message = \"Can you explain me layer normalization in python and how it is used in LLMs.\"\n",
    "\n",
    "# Format into chat template\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": user_message}\n",
    "]\n",
    "model_input = tok.apply_chat_template(messages, tokenize=True, return_tensors=\"pt\").to(m.device)\n",
    "\n",
    "# Generate\n",
    "output_ids = m.generate(model_input, max_new_tokens=2000, temperature=0.2, do_sample=False)\n",
    "\n",
    "# Decode full output\n",
    "decoded = tok.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Extract assistant reply (after the last \"Assistant:\" marker if present)\n",
    "if \"Assistant:\" in decoded:\n",
    "    assistant_reply = decoded.split(\"Assistant:\")[-1].strip()\n",
    "else:\n",
    "    assistant_reply = decoded.strip()\n",
    "\n",
    "# Print parts separately\n",
    "print(\"System prompt:\", system_prompt)\n",
    "print(\"User message:\", user_message)\n",
    "print(\"Assistant reply:\", assistant_reply)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "728ea739-bee7-4d59-9c60-c2c221ee4a31",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
